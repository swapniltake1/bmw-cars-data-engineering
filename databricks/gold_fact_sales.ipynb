{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e43835b-b7de-4487-95b3-261ed72b2a95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for Delta Lake operations\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, max as spark_max\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4fd73a5-b264-4716-9b62-19fdf236ae83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Set Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter to control initial vs incremental load\n",
    "# '0' = Initial load (first time)\n",
    "# '1' = Incremental load (subsequent runs)\n",
    "incremental_flag = '0'\n",
    "\n",
    "print(f\"Load Type: {'Initial Load' if incremental_flag == '0' else 'Incremental Load'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404dd84d-fce9-40ac-ac7d-4beae5426d5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Read Source Data from Bronze Layer"
    }
   },
   "outputs": [],
   "source": [
    "# Read the source sales data from parquet files\n",
    "# This contains all the transactional sales data\n",
    "df_source = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Branch_ID,\n",
    "        Dealer_ID,\n",
    "        Model_ID,\n",
    "        Revenue,\n",
    "        Units_Sold,\n",
    "        Year,\n",
    "        Month,\n",
    "        Day\n",
    "    FROM parquet.`abfss://silver@swap01storageaccount.dfs.core.windows.net/carsalesdata`\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Source data loaded: {df_source.count()} records\")\n",
    "df_source.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74539b95-f8bd-4f82-9b8d-e2c56e7f0349",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Create Date Column for Joining"
    }
   },
   "outputs": [],
   "source": [
    "# Create a proper date column from Year, Month, Day\n",
    "# This will be used to join with the date dimension\n",
    "from pyspark.sql.functions import to_date, concat_ws, lpad\n",
    "\n",
    "df_source = df_source.withColumn(\n",
    "    'Date', \n",
    "    to_date(concat_ws('-', col('Year'), lpad(col('Month'), 2, '0'), lpad(col('Day'), 2, '0')))\n",
    ")\n",
    "\n",
    "print(\"✓ Date column created\")\n",
    "df_source.select('Year', 'Month', 'Day', 'Date').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bb3c57-41cb-4560-8d9c-16e118a59a87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Join with Branch Dimension to Get Surrogate Key"
    }
   },
   "outputs": [],
   "source": [
    "# Join with branch dimension table to get dim_branch_key\n",
    "# This replaces the business key (Branch_ID) with surrogate key\n",
    "df_branch = spark.table('bmw_cars_catalog.gold.branch')\n",
    "\n",
    "df_with_branch = df_source.join(\n",
    "    df_branch.select('Branch_ID', 'dim_branch_key'),\n",
    "    on='Branch_ID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Joined with Branch dimension: {df_with_branch.count()} records\")\n",
    "df_with_branch.select('Branch_ID', 'dim_branch_key', 'Revenue', 'Units_Sold').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57466b79-2568-49ca-8f9d-08d8718db7e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Join with Dealer Dimension to Get Surrogate Key"
    }
   },
   "outputs": [],
   "source": [
    "# Join with dealer dimension table to get dim_dealer_key\n",
    "# This replaces the business key (Dealer_ID) with surrogate key\n",
    "df_dealer = spark.table('bmw_cars_catalog.gold.dealer')\n",
    "\n",
    "df_with_dealer = df_with_branch.join(\n",
    "    df_dealer.select('Dealer_ID', 'dim_dealer_key'),\n",
    "    on='Dealer_ID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Joined with Dealer dimension: {df_with_dealer.count()} records\")\n",
    "df_with_dealer.select('Dealer_ID', 'dim_dealer_key', 'Branch_ID', 'dim_branch_key').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff15c06-feb0-4ea1-9888-77fb4d4e354d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Join with Model Dimension to Get Surrogate Key"
    }
   },
   "outputs": [],
   "source": [
    "# Join with model dimension table to get dim_model_key\n",
    "# This replaces the business key (Model_ID) with surrogate key\n",
    "df_model = spark.table('bmw_cars_catalog.gold.model')\n",
    "\n",
    "df_with_model = df_with_dealer.join(\n",
    "    df_model.select('Model_ID', 'dim_model_key'),\n",
    "    on='Model_ID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Joined with Model dimension: {df_with_model.count()} records\")\n",
    "df_with_model.select('Model_ID', 'dim_model_key', 'Revenue', 'Units_Sold').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071dcb71-7e6c-47cd-904a-7174ff0e4732",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 8: Join with Date Dimension to Get Surrogate Key"
    }
   },
   "outputs": [],
   "source": [
    "# Join with date dimension table to get dim_date_key\n",
    "# This replaces the date with surrogate key\n",
    "df_date = spark.table('bmw_cars_catalog.gold.dim_date')\n",
    "\n",
    "df_with_date = df_with_model.join(\n",
    "    df_date.select('Date', 'dim_date_key'),\n",
    "    on='Date',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Joined with Date dimension: {df_with_date.count()} records\")\n",
    "df_with_date.select('Date', 'dim_date_key', 'Revenue', 'Units_Sold').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1195d3-fb2c-4bd0-a95d-3ca519fd54da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 9: Create Final Fact Table Structure"
    }
   },
   "outputs": [],
   "source": [
    "# Select only the columns needed for the fact table:\n",
    "# - Surrogate keys from all dimensions\n",
    "# - Measures (Revenue, Units_Sold)\n",
    "df_fact = df_with_date.select(\n",
    "    col('dim_branch_key'),\n",
    "    col('dim_dealer_key'),\n",
    "    col('dim_model_key'),\n",
    "    col('dim_date_key'),\n",
    "    col('Revenue'),\n",
    "    col('Units_Sold')\n",
    ")\n",
    "\n",
    "print(f\"✓ Fact table structure created: {df_fact.count()} records\")\n",
    "print(\"\\nFact Table Schema:\")\n",
    "df_fact.printSchema()\n",
    "df_fact.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb3f16e-78ac-4453-b710-b9775e59ed5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 10: Add Fact Surrogate Key"
    }
   },
   "outputs": [],
   "source": [
    "# Add a surrogate key for the fact table itself\n",
    "# This uniquely identifies each sales transaction\n",
    "\n",
    "if incremental_flag == '0':\n",
    "    # Initial load: start from 1\n",
    "    max_fact_key = 1\n",
    "else:\n",
    "    # Incremental load: get the max existing key\n",
    "    max_fact_key = spark.sql('SELECT max(fact_sales_key) FROM bmw_cars_catalog.gold.fact_sales').collect()[0][0]\n",
    "    if max_fact_key is None:\n",
    "        max_fact_key = 1\n",
    "\n",
    "df_fact = df_fact.withColumn('fact_sales_key', max_fact_key + monotonically_increasing_id())\n",
    "\n",
    "print(f\"✓ Fact surrogate key added (starting from {max_fact_key})\")\n",
    "df_fact.select('fact_sales_key', 'dim_branch_key', 'dim_dealer_key', 'dim_model_key', 'dim_date_key', 'Revenue', 'Units_Sold').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6619fda-d4fd-475c-a61a-4be561949046",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 11: Load Fact Table (Initial or Incremental)"
    }
   },
   "outputs": [],
   "source": [
    "# Load the fact table into Delta Lake\n",
    "# Initial load: Create the table\n",
    "# Incremental load: Append new records\n",
    "\n",
    "if spark.catalog.tableExists('bmw_cars_catalog.gold.fact_sales'):\n",
    "    # Incremental load: Append new data\n",
    "    df_fact.write.format(\"delta\") \\\n",
    "        .mode('append') \\\n",
    "        .option('path', 'abfss://silver@swap01storageaccount.dfs.core.windows.net/fact_sales') \\\n",
    "        .saveAsTable('bmw_cars_catalog.gold.fact_sales')\n",
    "    print(\"✓ Incremental load completed - New records appended\")\n",
    "else:\n",
    "    # Initial load: Create table\n",
    "    df_fact.write.format(\"delta\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('path', 'abfss://silver@swap01storageaccount.dfs.core.windows.net/fact_sales') \\\n",
    "        .saveAsTable('bmw_cars_catalog.gold.fact_sales')\n",
    "    print(\"✓ Initial load completed - Fact table created\")\n",
    "\n",
    "print(f\"\\nTotal records in fact table: {spark.table('bmw_cars_catalog.gold.fact_sales').count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c90056-0fc3-4bf5-94db-c79039ede1a5",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771136724310}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Step 12: Verify Fact Table"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the fact table by joining back with dimensions\n",
    "# This shows the complete business view\n",
    "\n",
    "fact_verification = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        f.fact_sales_key,\n",
    "        b.BranchName,\n",
    "        d.DealerName,\n",
    "        m.Model_Category,\n",
    "        dt.Date,\n",
    "        dt.DayName,\n",
    "        dt.MonthName,\n",
    "        dt.Year,\n",
    "        f.Revenue,\n",
    "        f.Units_Sold\n",
    "    FROM bmw_cars_catalog.gold.fact_sales f\n",
    "    INNER JOIN bmw_cars_catalog.gold.branch b ON f.dim_branch_key = b.dim_branch_key\n",
    "    INNER JOIN bmw_cars_catalog.gold.dealer d ON f.dim_dealer_key = d.dim_dealer_key\n",
    "    INNER JOIN bmw_cars_catalog.gold.model m ON f.dim_model_key = m.dim_model_key\n",
    "    INNER JOIN bmw_cars_catalog.gold.dim_date dt ON f.dim_date_key = dt.dim_date_key\n",
    "    ORDER BY f.fact_sales_key\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Fact table verification - Sample records with dimension details:\")\n",
    "fact_verification.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1689a66d-aaec-458b-a2d4-b44cec3ae0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_fact_sales",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
